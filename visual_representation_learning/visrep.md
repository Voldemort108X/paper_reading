# Visual representation learning

| Title | Motivation | Method | Conclusion | Application | Year | Limitation | Comment |
| - | - | - | - | - | - | - | - |
| A Simple Framework for Contrastive Learning of Visual Representations [[Paper]](https://arxiv.org/pdf/2002.05709.pdf) | Simply recently proposed contrastive self-supervised learning algorithms without specialized architectures of a memory bank. | Input, Data augmentation, Base encoder, Projection head, Contrastive loss | 1) Composition of data augmentation plays a critical role. 2) Introducing a learnable transformation (projection head) substantially improves the quality of learned representations. 3) Contrastive learning benefits from larger batch sizes and more training steps compared with supervised learning. | Image classification | 2020 | | A very useful blog to explain the paper at [[here]](https://amitness.com/2020/03/illustrated-simclr/)
| Momentum contrast for unsupervised visual representation learning [[Paper]](https://arxiv.org/pdf/1911.05722.pdf)| 
| Dimensionality Reduction by Learning an Invariant Mapping [[Paper]](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf) | 
| Supervised Contrastive Learning [[Paper]](https://arxiv.org/pdf/2004.11362.pdf) | Extend the self-supervised batch contrastive approach to fully-supervised setting. 
| FaceNet: A Unified Embedding for Face Recognition and Clustering [[Paper]](https://arxiv.org/pdf/1503.03832.pdf)
| Large-Margin Softmax Loss for Convolutional Neural Networks [[Paper]](https://arxiv.org/pdf/1612.02295.pdf)
| Improved Deep Metric Learning with Multi-class N-pair Loss Objective [[Paper]](https://papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective.pdf)